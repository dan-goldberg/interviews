{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dangoldberg/miniconda3/envs/tbj/lib/python3.6/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: Loading required package: Matrix\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['lme4', 'Matrix', 'tools', 'stats', 'graphics', 'grDevices',\n",
       "       'utils', 'datasets', 'methods', 'base'], dtype='<U9')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%R library(lme4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "data(dietox, package='geepack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dangoldberg/miniconda3/envs/tbj/lib/python3.6/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: singular fit\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by REML ['lmerMod']\n",
       "Formula: Weight ~ Pig + (1 | Pig)\n",
       "   Data: dietox\n",
       "\n",
       "REML criterion at convergence: 7995.2\n",
       "\n",
       "Scaled residuals: \n",
       "     Min       1Q   Median       3Q      Max \n",
       "-1.83089 -0.89860 -0.06234  0.81915  2.25007 \n",
       "\n",
       "Random effects:\n",
       " Groups   Name        Variance Std.Dev.\n",
       " Pig      (Intercept)   0.0     0.00   \n",
       " Residual             624.7    24.99   \n",
       "Number of obs: 861, groups:  Pig, 72\n",
       "\n",
       "Fixed effects:\n",
       "              Estimate Std. Error t value\n",
       "(Intercept)  6.087e+01  4.105e+00  14.827\n",
       "Pig         -2.334e-05  6.438e-04  -0.036\n",
       "\n",
       "Correlation of Fixed Effects:\n",
       "    (Intr)\n",
       "Pig -0.978\n",
       "fit warnings:\n",
       "Some predictor variables are on very different scales: consider rescaling\n",
       "convergence code: 0\n",
       "singular fit\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%R print(summary(lmer('Weight ~ Pig + (1|Pig)', data=dietox)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "========\n",
      "minimize\n",
      "========\n",
      "\n",
      "\n",
      "bfgs\n",
      "====\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "BFGS algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "norm : float\n",
      "    Order of norm (Inf is max, -Inf is min).\n",
      "eps : float or ndarray\n",
      "    If `jac` is approximated, use this value for the step size.\n",
      "\n",
      "cg\n",
      "==\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "conjugate gradient algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "norm : float\n",
      "    Order of norm (Inf is max, -Inf is min).\n",
      "eps : float or ndarray\n",
      "    If `jac` is approximated, use this value for the step size.\n",
      "\n",
      "cobyla\n",
      "======\n",
      "\n",
      "Minimize a scalar function of one or more variables using the\n",
      "Constrained Optimization BY Linear Approximation (COBYLA) algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "rhobeg : float\n",
      "    Reasonable initial changes to the variables.\n",
      "tol : float\n",
      "    Final accuracy in the optimization (not precisely guaranteed).\n",
      "    This is a lower bound on the size of the trust region.\n",
      "disp : bool\n",
      "    Set to True to print convergence messages. If False,\n",
      "    `verbosity` is ignored as set to 0.\n",
      "maxiter : int\n",
      "    Maximum number of function evaluations.\n",
      "catol : float\n",
      "    Tolerance (absolute) for constraint violations\n",
      "\n",
      "dogleg\n",
      "======\n",
      "\n",
      "Minimization of scalar function of one or more variables using\n",
      "the dog-leg trust-region algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "initial_trust_radius : float\n",
      "    Initial trust-region radius.\n",
      "max_trust_radius : float\n",
      "    Maximum value of the trust-region radius. No steps that are longer\n",
      "    than this value will be proposed.\n",
      "eta : float\n",
      "    Trust region related acceptance stringency for proposed steps.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "\n",
      "l-bfgs-b\n",
      "========\n",
      "\n",
      "Minimize a scalar function of one or more variables using the L-BFGS-B\n",
      "algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "   Set to True to print convergence messages.\n",
      "maxcor : int\n",
      "    The maximum number of variable metric corrections used to\n",
      "    define the limited memory matrix. (The limited memory BFGS\n",
      "    method does not store the full hessian but uses this many terms\n",
      "    in an approximation to it.)\n",
      "ftol : float\n",
      "    The iteration stops when ``(f^k -\n",
      "    f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.\n",
      "gtol : float\n",
      "    The iteration will stop when ``max{|proj g_i | i = 1, ..., n}\n",
      "    <= gtol`` where ``pg_i`` is the i-th component of the\n",
      "    projected gradient.\n",
      "eps : float\n",
      "    Step size used for numerical approximation of the jacobian.\n",
      "disp : int\n",
      "    Set to True to print convergence messages.\n",
      "maxfun : int\n",
      "    Maximum number of function evaluations.\n",
      "maxiter : int\n",
      "    Maximum number of iterations.\n",
      "maxls : int, optional\n",
      "    Maximum number of line search steps (per iteration). Default is 20.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The option `ftol` is exposed via the `scipy.optimize.minimize` interface,\n",
      "but calling `scipy.optimize.fmin_l_bfgs_b` directly exposes `factr`. The\n",
      "relationship between the two is ``ftol = factr * numpy.finfo(float).eps``.\n",
      "I.e., `factr` multiplies the default machine floating-point precision to\n",
      "arrive at `ftol`.\n",
      "\n",
      "nelder-mead\n",
      "===========\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "Nelder-Mead algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter, maxfev : int\n",
      "    Maximum allowed number of iterations and function evaluations.\n",
      "    Will default to ``N*200``, where ``N`` is the number of\n",
      "    variables, if neither `maxiter` or `maxfev` is set. If both\n",
      "    `maxiter` and `maxfev` are set, minimization will stop at the\n",
      "    first reached.\n",
      "initial_simplex : array_like of shape (N + 1, N)\n",
      "    Initial simplex. If given, overrides `x0`.\n",
      "    ``initial_simplex[j,:]`` should contain the coordinates of\n",
      "    the j-th vertex of the ``N+1`` vertices in the simplex, where\n",
      "    ``N`` is the dimension.\n",
      "xatol : float, optional\n",
      "    Absolute error in xopt between iterations that is acceptable for\n",
      "    convergence.\n",
      "fatol : number, optional\n",
      "    Absolute error in func(xopt) between iterations that is acceptable for\n",
      "    convergence.\n",
      "adaptive : bool, optional\n",
      "    Adapt algorithm parameters to dimensionality of problem. Useful for\n",
      "    high-dimensional minimization [1]_.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] Gao, F. and Han, L.\n",
      "   Implementing the Nelder-Mead simplex algorithm with adaptive\n",
      "   parameters. 2012. Computational Optimization and Applications.\n",
      "   51:1, pp. 259-277\n",
      "\n",
      "newton-cg\n",
      "=========\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "Newton-CG algorithm.\n",
      "\n",
      "Note that the `jac` parameter (Jacobian) is required.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "xtol : float\n",
      "    Average relative error in solution `xopt` acceptable for\n",
      "    convergence.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "eps : float or ndarray\n",
      "    If `jac` is approximated, use this value for the step size.\n",
      "\n",
      "powell\n",
      "======\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "modified Powell algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "xtol : float\n",
      "    Relative error in solution `xopt` acceptable for convergence.\n",
      "ftol : float\n",
      "    Relative error in ``fun(xopt)`` acceptable for convergence.\n",
      "maxiter, maxfev : int\n",
      "    Maximum allowed number of iterations and function evaluations.\n",
      "    Will default to ``N*1000``, where ``N`` is the number of\n",
      "    variables, if neither `maxiter` or `maxfev` is set. If both\n",
      "    `maxiter` and `maxfev` are set, minimization will stop at the\n",
      "    first reached.\n",
      "direc : ndarray\n",
      "    Initial set of direction vectors for the Powell method.\n",
      "\n",
      "slsqp\n",
      "=====\n",
      "\n",
      "Minimize a scalar function of one or more variables using Sequential\n",
      "Least SQuares Programming (SLSQP).\n",
      "\n",
      "Options\n",
      "-------\n",
      "ftol : float\n",
      "    Precision goal for the value of f in the stopping criterion.\n",
      "eps : float\n",
      "    Step size used for numerical approximation of the Jacobian.\n",
      "disp : bool\n",
      "    Set to True to print convergence messages. If False,\n",
      "    `verbosity` is ignored and set to 0.\n",
      "maxiter : int\n",
      "    Maximum number of iterations.\n",
      "\n",
      "tnc\n",
      "===\n",
      "\n",
      "Minimize a scalar function of one or more variables using a truncated\n",
      "Newton (TNC) algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "eps : float\n",
      "    Step size used for numerical approximation of the jacobian.\n",
      "scale : list of floats\n",
      "    Scaling factors to apply to each variable.  If None, the\n",
      "    factors are up-low for interval bounded variables and\n",
      "    1+|x] fo the others.  Defaults to None\n",
      "offset : float\n",
      "    Value to subtract from each variable.  If None, the\n",
      "    offsets are (up+low)/2 for interval bounded variables\n",
      "    and x for the others.\n",
      "disp : bool\n",
      "   Set to True to print convergence messages.\n",
      "maxCGit : int\n",
      "    Maximum number of hessian*vector evaluations per main\n",
      "    iteration.  If maxCGit == 0, the direction chosen is\n",
      "    -gradient if maxCGit < 0, maxCGit is set to\n",
      "    max(1,min(50,n/2)).  Defaults to -1.\n",
      "maxiter : int\n",
      "    Maximum number of function evaluation.  if None, `maxiter` is\n",
      "    set to max(100, 10*len(x0)).  Defaults to None.\n",
      "eta : float\n",
      "    Severity of the line search. if < 0 or > 1, set to 0.25.\n",
      "    Defaults to -1.\n",
      "stepmx : float\n",
      "    Maximum step for the line search.  May be increased during\n",
      "    call.  If too small, it will be set to 10.0.  Defaults to 0.\n",
      "accuracy : float\n",
      "    Relative precision for finite difference calculations.  If\n",
      "    <= machine_precision, set to sqrt(machine_precision).\n",
      "    Defaults to 0.\n",
      "minfev : float\n",
      "    Minimum function value estimate.  Defaults to 0.\n",
      "ftol : float\n",
      "    Precision goal for the value of f in the stopping criterion.\n",
      "    If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
      "xtol : float\n",
      "    Precision goal for the value of x in the stopping\n",
      "    criterion (after applying x scaling factors).  If xtol <\n",
      "    0.0, xtol is set to sqrt(machine_precision).  Defaults to\n",
      "    -1.\n",
      "gtol : float\n",
      "    Precision goal for the value of the projected gradient in\n",
      "    the stopping criterion (after applying x scaling factors).\n",
      "    If gtol < 0.0, gtol is set to 1e-2 * sqrt(accuracy).\n",
      "    Setting it to 0.0 is not recommended.  Defaults to -1.\n",
      "rescale : float\n",
      "    Scaling factor (in log10) used to trigger f value\n",
      "    rescaling.  If 0, rescale at each iteration.  If a large\n",
      "    value, never rescale.  If < 0, rescale is set to 1.3.\n",
      "\n",
      "trust-ncg\n",
      "=========\n",
      "\n",
      "Minimization of scalar function of one or more variables using\n",
      "the Newton conjugate gradient trust-region algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "initial_trust_radius : float\n",
      "    Initial trust-region radius.\n",
      "max_trust_radius : float\n",
      "    Maximum value of the trust-region radius. No steps that are longer\n",
      "    than this value will be proposed.\n",
      "eta : float\n",
      "    Trust region related acceptance stringency for proposed steps.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "\n",
      "===============\n",
      "minimize_scalar\n",
      "===============\n",
      "\n",
      "\n",
      "brent\n",
      "=====\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "xtol : float\n",
      "    Relative error in solution `xopt` acceptable for convergence.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Uses inverse parabolic interpolation when possible to speed up\n",
      "convergence of golden section method.\n",
      "\n",
      "bounded\n",
      "=======\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "disp: int, optional\n",
      "    If non-zero, print messages.\n",
      "        0 : no message printing.\n",
      "        1 : non-convergence notification messages only.\n",
      "        2 : print a message on convergence too.\n",
      "        3 : print iteration results.\n",
      "xatol : float\n",
      "    Absolute error in solution `xopt` acceptable for convergence.\n",
      "\n",
      "golden\n",
      "======\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "xtol : float\n",
      "    Relative error in solution `xopt` acceptable for convergence.\n",
      "\n",
      "\n",
      "====\n",
      "root\n",
      "====\n",
      "\n",
      "\n",
      "hybr\n",
      "====\n",
      "\n",
      "Find the roots of a multivariate function using MINPACK's hybrd and\n",
      "hybrj routines (modified Powell method).\n",
      "\n",
      "Options\n",
      "-------\n",
      "col_deriv : bool\n",
      "    Specify whether the Jacobian function computes derivatives down\n",
      "    the columns (faster, because there is no transpose operation).\n",
      "xtol : float\n",
      "    The calculation will terminate if the relative error between two\n",
      "    consecutive iterates is at most `xtol`.\n",
      "maxfev : int\n",
      "    The maximum number of calls to the function. If zero, then\n",
      "    ``100*(N+1)`` is the maximum where N is the number of elements\n",
      "    in `x0`.\n",
      "band : tuple\n",
      "    If set to a two-sequence containing the number of sub- and\n",
      "    super-diagonals within the band of the Jacobi matrix, the\n",
      "    Jacobi matrix is considered banded (only for ``fprime=None``).\n",
      "eps : float\n",
      "    A suitable step length for the forward-difference\n",
      "    approximation of the Jacobian (for ``fprime=None``). If\n",
      "    `eps` is less than the machine precision, it is assumed\n",
      "    that the relative errors in the functions are of the order of\n",
      "    the machine precision.\n",
      "factor : float\n",
      "    A parameter determining the initial step bound\n",
      "    (``factor * || diag * x||``).  Should be in the interval\n",
      "    ``(0.1, 100)``.\n",
      "diag : sequence\n",
      "    N positive entries that serve as a scale factors for the\n",
      "    variables.\n",
      "\n",
      "lm\n",
      "==\n",
      "\n",
      "Solve for least squares with Levenberg-Marquardt\n",
      "\n",
      "Options\n",
      "-------\n",
      "col_deriv : bool\n",
      "    non-zero to specify that the Jacobian function computes derivatives\n",
      "    down the columns (faster, because there is no transpose operation).\n",
      "ftol : float\n",
      "    Relative error desired in the sum of squares.\n",
      "xtol : float\n",
      "    Relative error desired in the approximate solution.\n",
      "gtol : float\n",
      "    Orthogonality desired between the function vector and the columns\n",
      "    of the Jacobian.\n",
      "maxiter : int\n",
      "    The maximum number of calls to the function. If zero, then\n",
      "    100*(N+1) is the maximum where N is the number of elements in x0.\n",
      "epsfcn : float\n",
      "    A suitable step length for the forward-difference approximation of\n",
      "    the Jacobian (for Dfun=None). If epsfcn is less than the machine\n",
      "    precision, it is assumed that the relative errors in the functions\n",
      "    are of the order of the machine precision.\n",
      "factor : float\n",
      "    A parameter determining the initial step bound\n",
      "    (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
      "diag : sequence\n",
      "    N positive entries that serve as a scale factors for the variables.\n",
      "\n",
      "broyden1\n",
      "========\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden\n",
      "            matrix stays low. Can either be a string giving the\n",
      "            name of the method, or a tuple of the form ``(method,\n",
      "            param1, param2, ...)`` that gives the name of the\n",
      "            method and values for additional parameters.\n",
      "\n",
      "            Methods available:\n",
      "                - ``restart``: drop all matrix columns. Has no\n",
      "                    extra parameters.\n",
      "                - ``simple``: drop oldest matrix column. Has no\n",
      "                    extra parameters.\n",
      "                - ``svd``: keep only the most significant SVD\n",
      "                    components.\n",
      "                  Extra parameters:\n",
      "                      - ``to_retain``: number of SVD components to\n",
      "                          retain when rank reduction is done.\n",
      "                          Default is ``max_rank - 2``.\n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (ie., no rank reduction).\n",
      "\n",
      "broyden2\n",
      "========\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        Initial guess for the Jacobian is (-1/alpha).\n",
      "    reduction_method : str or tuple, optional\n",
      "        Method used in ensuring that the rank of the Broyden\n",
      "        matrix stays low. Can either be a string giving the\n",
      "        name of the method, or a tuple of the form ``(method,\n",
      "        param1, param2, ...)`` that gives the name of the\n",
      "        method and values for additional parameters.\n",
      "\n",
      "        Methods available:\n",
      "            - ``restart``: drop all matrix columns. Has no\n",
      "                extra parameters.\n",
      "            - ``simple``: drop oldest matrix column. Has no\n",
      "                extra parameters.\n",
      "            - ``svd``: keep only the most significant SVD\n",
      "                components.\n",
      "              Extra parameters:\n",
      "                  - ``to_retain``: number of SVD components to\n",
      "                      retain when rank reduction is done.\n",
      "                      Default is ``max_rank - 2``.\n",
      "    max_rank : int, optional\n",
      "        Maximum rank for the Broyden matrix.\n",
      "        Default is infinity (ie., no rank reduction).\n",
      "\n",
      "anderson\n",
      "========\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        Initial guess for the Jacobian is (-1/alpha).\n",
      "    M : float, optional\n",
      "        Number of previous vectors to retain. Defaults to 5.\n",
      "    w0 : float, optional\n",
      "        Regularization parameter for numerical stability.\n",
      "        Compared to unity, good values of the order of 0.01.\n",
      "\n",
      "diagbroyden\n",
      "===========\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        initial guess for the jacobian is (-1/alpha).\n",
      "\n",
      "excitingmixing\n",
      "==============\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        Initial Jacobian approximation is (-1/alpha).\n",
      "    alphamax : float, optional\n",
      "        The entries of the diagonal Jacobian are kept in the range\n",
      "        ``[alpha, alphamax]``.\n",
      "\n",
      "linearmixing\n",
      "============\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, ``NoConvergence`` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        initial guess for the jacobian is (-1/alpha).\n",
      "\n",
      "krylov\n",
      "======\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    rdiff : float, optional\n",
      "        Relative step size to use in numerical differentiation.\n",
      "    method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or function\n",
      "        Krylov method to use to approximate the Jacobian.\n",
      "        Can be a string, or a function implementing the same\n",
      "        interface as the iterative solvers in\n",
      "        `scipy.sparse.linalg`.\n",
      "\n",
      "        The default is `scipy.sparse.linalg.lgmres`.\n",
      "    inner_M : LinearOperator or InverseJacobian\n",
      "        Preconditioner for the inner Krylov iteration.\n",
      "        Note that you can use also inverse Jacobians as (adaptive)\n",
      "        preconditioners. For example,\n",
      "\n",
      "        >>> jac = BroydenFirst()\n",
      "        >>> kjac = KrylovJacobian(inner_M=jac.inverse).\n",
      "\n",
      "        If the preconditioner has a method named 'update', it will\n",
      "        be called as ``update(x, f)`` after each nonlinear step,\n",
      "        with ``x`` giving the current point, and ``f`` the current\n",
      "        function value.\n",
      "    inner_tol, inner_maxiter, ...\n",
      "        Parameters to pass on to the \"inner\" Krylov solver.\n",
      "        See `scipy.sparse.linalg.gmres` for details.\n",
      "    outer_k : int, optional\n",
      "        Size of the subspace kept across LGMRES nonlinear\n",
      "        iterations.\n",
      "\n",
      "        See `scipy.sparse.linalg.lgmres` for details.\n",
      "\n",
      "df-sane\n",
      "=======\n",
      "\n",
      "Solve nonlinear equation with the DF-SANE method\n",
      "\n",
      "Options\n",
      "-------\n",
      "ftol : float, optional\n",
      "    Relative norm tolerance.\n",
      "fatol : float, optional\n",
      "    Absolute norm tolerance.\n",
      "    Algorithm terminates when ``||func(x)|| < fatol + ftol ||func(x_0)||``.\n",
      "fnorm : callable, optional\n",
      "    Norm to use in the convergence check. If None, 2-norm is used.\n",
      "maxfev : int, optional\n",
      "    Maximum number of function evaluations.\n",
      "disp : bool, optional\n",
      "    Whether to print convergence process to stdout.\n",
      "eta_strategy : callable, optional\n",
      "    Choice of the ``eta_k`` parameter, which gives slack for growth\n",
      "    of ``||F||**2``.  Called as ``eta_k = eta_strategy(k, x, F)`` with\n",
      "    `k` the iteration number, `x` the current iterate and `F` the current\n",
      "    residual. Should satisfy ``eta_k > 0`` and ``sum(eta, k=0..inf) < inf``.\n",
      "    Default: ``||F||**2 / (1 + k)**2``.\n",
      "sigma_eps : float, optional\n",
      "    The spectral coefficient is constrained to ``sigma_eps < sigma < 1/sigma_eps``.\n",
      "    Default: 1e-10\n",
      "sigma_0 : float, optional\n",
      "    Initial spectral coefficient.\n",
      "    Default: 1.0\n",
      "M : int, optional\n",
      "    Number of iterates to include in the nonmonotonic line search.\n",
      "    Default: 10\n",
      "line_search : {'cruz', 'cheng'}\n",
      "    Type of line search to employ. 'cruz' is the original one defined in\n",
      "    [Martinez & Raydan. Math. Comp. 75, 1429 (2006)], 'cheng' is\n",
      "    a modified search defined in [Cheng & Li. IMA J. Numer. Anal. 29, 814 (2009)].\n",
      "    Default: 'cruz'\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] \"Spectral residual method without gradient information for solving\n",
      "       large-scale nonlinear systems of equations.\" W. La Cruz,\n",
      "       J.M. Martinez, M. Raydan. Math. Comp. **75**, 1429 (2006).\n",
      ".. [2] W. La Cruz, Opt. Meth. Software, 29, 24 (2014).\n",
      ".. [3] W. Cheng, D.-H. Li. IMA J. Numer. Anal. **29**, 814 (2009).\n",
      "\n",
      "\n",
      "=======\n",
      "linprog\n",
      "=======\n",
      "\n",
      "\n",
      "simplex\n",
      "=======\n",
      "\n",
      "Solve the following linear programming problem via a two-phase\n",
      "simplex algorithm.::\n",
      "\n",
      "    minimize:     c^T * x\n",
      "\n",
      "    subject to:   A_ub * x <= b_ub\n",
      "                  A_eq * x == b_eq\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "c : array_like\n",
      "    Coefficients of the linear objective function to be minimized.\n",
      "A_ub : array_like\n",
      "    2-D array which, when matrix-multiplied by ``x``, gives the values of\n",
      "    the upper-bound inequality constraints at ``x``.\n",
      "b_ub : array_like\n",
      "    1-D array of values representing the upper-bound of each inequality\n",
      "    constraint (row) in ``A_ub``.\n",
      "A_eq : array_like\n",
      "    2-D array which, when matrix-multiplied by ``x``, gives the values of\n",
      "    the equality constraints at ``x``.\n",
      "b_eq : array_like\n",
      "    1-D array of values representing the RHS of each equality constraint\n",
      "    (row) in ``A_eq``.\n",
      "bounds : array_like\n",
      "    The bounds for each independent variable in the solution, which can\n",
      "    take one of three forms::\n",
      "\n",
      "    None : The default bounds, all variables are non-negative.\n",
      "    (lb, ub) : If a 2-element sequence is provided, the same\n",
      "              lower bound (lb) and upper bound (ub) will be applied\n",
      "              to all variables.\n",
      "    [(lb_0, ub_0), (lb_1, ub_1), ...] : If an n x 2 sequence is provided,\n",
      "              each variable x_i will be bounded by lb[i] and ub[i].\n",
      "    Infinite bounds are specified using -np.inf (negative)\n",
      "    or np.inf (positive).\n",
      "\n",
      "callback : callable\n",
      "    If a callback function is provide, it will be called within each\n",
      "    iteration of the simplex algorithm. The callback must have the\n",
      "    signature ``callback(xk, **kwargs)`` where ``xk`` is the current s\n",
      "    olution vector and kwargs is a dictionary containing the following::\n",
      "\n",
      "    \"tableau\" : The current Simplex algorithm tableau\n",
      "    \"nit\" : The current iteration.\n",
      "    \"pivot\" : The pivot (row, column) used for the next iteration.\n",
      "    \"phase\" : Whether the algorithm is in Phase 1 or Phase 2.\n",
      "    \"bv\" : A structured array containing a string representation of each\n",
      "           basic variable and its current value.\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "   The maximum number of iterations to perform.\n",
      "disp : bool\n",
      "    If True, print exit status message to sys.stdout\n",
      "tol : float\n",
      "    The tolerance which determines when a solution is \"close enough\" to\n",
      "    zero in Phase 1 to be considered a basic feasible solution or close\n",
      "    enough to positive to serve as an optimal solution.\n",
      "bland : bool\n",
      "    If True, use Bland's anti-cycling rule [3] to choose pivots to\n",
      "    prevent cycling.  If False, choose pivots which should lead to a\n",
      "    converged solution more quickly.  The latter method is subject to\n",
      "    cycling (non-convergence) in rare instances.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "A `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "\n",
      "    x : ndarray\n",
      "        The independent variable vector which optimizes the linear\n",
      "        programming problem.\n",
      "    fun : float\n",
      "        Value of the objective function.\n",
      "    slack : ndarray\n",
      "        The values of the slack variables.  Each slack variable corresponds\n",
      "        to an inequality constraint.  If the slack is zero, then the\n",
      "        corresponding constraint is active.\n",
      "    success : bool\n",
      "        Returns True if the algorithm succeeded in finding an optimal\n",
      "        solution.\n",
      "    status : int\n",
      "        An integer representing the exit status of the optimization::\n",
      "\n",
      "         0 : Optimization terminated successfully\n",
      "         1 : Iteration limit reached\n",
      "         2 : Problem appears to be infeasible\n",
      "         3 : Problem appears to be unbounded\n",
      "\n",
      "    nit : int\n",
      "        The number of iterations performed.\n",
      "    message : str\n",
      "        A string descriptor of the exit status of the optimization.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Consider the following problem:\n",
      "\n",
      "Minimize: f = -1*x[0] + 4*x[1]\n",
      "\n",
      "Subject to: -3*x[0] + 1*x[1] <= 6\n",
      "             1*x[0] + 2*x[1] <= 4\n",
      "                        x[1] >= -3\n",
      "\n",
      "where:  -inf <= x[0] <= inf\n",
      "\n",
      "This problem deviates from the standard linear programming problem.  In\n",
      "standard form, linear programming problems assume the variables x are\n",
      "non-negative.  Since the variables don't have standard bounds where\n",
      "0 <= x <= inf, the bounds of the variables must be explicitly set.\n",
      "\n",
      "There are two upper-bound constraints, which can be expressed as\n",
      "\n",
      "dot(A_ub, x) <= b_ub\n",
      "\n",
      "The input for this problem is as follows:\n",
      "\n",
      ">>> from scipy.optimize import linprog\n",
      ">>> c = [-1, 4]\n",
      ">>> A = [[-3, 1], [1, 2]]\n",
      ">>> b = [6, 4]\n",
      ">>> x0_bnds = (None, None)\n",
      ">>> x1_bnds = (-3, None)\n",
      ">>> res = linprog(c, A, b, bounds=(x0_bnds, x1_bnds))\n",
      ">>> print(res)\n",
      "     fun: -22.0\n",
      " message: 'Optimization terminated successfully.'\n",
      "     nit: 1\n",
      "   slack: array([ 39.,   0.])\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 10.,  -3.])\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] Dantzig, George B., Linear programming and extensions. Rand\n",
      "       Corporation Research Study Princeton Univ. Press, Princeton, NJ,\n",
      "       1963\n",
      ".. [2] Hillier, S.H. and Lieberman, G.J. (1995), \"Introduction to\n",
      "       Mathematical Programming\", McGraw-Hill, Chapter 4.\n",
      ".. [3] Bland, Robert G. New finite pivoting rules for the simplex method.\n",
      "       Mathematics of Operations Research (2), 1977: pp. 103-107.\n",
      "\n",
      "interior-point\n",
      "==============\n",
      "\n",
      "Minimize a linear objective function subject to linear\n",
      "equality constraints, linear inequality constraints, and simple bounds\n",
      "using the interior point method of [1]_.\n",
      "\n",
      "Linear programming is intended to solve problems of the following form::\n",
      "\n",
      "    Minimize:     c^T * x\n",
      "\n",
      "    Subject to:   A_ub * x <= b_ub\n",
      "                  A_eq * x == b_eq\n",
      "                  bounds[i][0] < x_i < bounds[i][1]\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "c : array_like\n",
      "    Coefficients of the linear objective function to be minimized.\n",
      "A_ub : array_like, optional\n",
      "    2-D array which, when matrix-multiplied by ``x``, gives the values of\n",
      "    the upper-bound inequality constraints at ``x``.\n",
      "b_ub : array_like, optional\n",
      "    1-D array of values representing the upper-bound of each inequality\n",
      "    constraint (row) in ``A_ub``.\n",
      "A_eq : array_like, optional\n",
      "    2-D array which, when matrix-multiplied by ``x``, gives the values of\n",
      "    the equality constraints at ``x``.\n",
      "b_eq : array_like, optional\n",
      "    1-D array of values representing the right hand side of each equality\n",
      "    constraint (row) in ``A_eq``.\n",
      "bounds : sequence, optional\n",
      "    ``(min, max)`` pairs for each element in ``x``, defining\n",
      "    the bounds on that parameter. Use ``None`` for one of ``min`` or\n",
      "    ``max`` when there is no bound in that direction. By default\n",
      "    bounds are ``(0, None)`` (non-negative).\n",
      "    If a sequence containing a single tuple is provided, then ``min`` and\n",
      "    ``max`` will be applied to all variables in the problem.\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int (default = 1000)\n",
      "    The maximum number of iterations of the algorithm.\n",
      "disp : bool (default = False)\n",
      "    Set to ``True`` if indicators of optimization status are to be printed\n",
      "    to the console each iteration.\n",
      "tol : float (default = 1e-8)\n",
      "    Termination tolerance to be used for all termination criteria;\n",
      "    see [1]_ Section 4.5.\n",
      "alpha0 : float (default = 0.99995)\n",
      "    The maximal step size for Mehrota's predictor-corrector search\n",
      "    direction; see :math:`\\beta_{3}` of [1]_ Table 8.1.\n",
      "beta : float (default = 0.1)\n",
      "    The desired reduction of the path parameter :math:`\\mu` (see [3]_)\n",
      "    when Mehrota's predictor-corrector is not in use (uncommon).\n",
      "sparse : bool (default = False)\n",
      "    Set to ``True`` if the problem is to be treated as sparse after\n",
      "    presolve. If either ``A_eq`` or ``A_ub`` is a sparse matrix,\n",
      "    this option will automatically be set ``True``, and the problem\n",
      "    will be treated as sparse even during presolve. If your constraint\n",
      "    matrices contain mostly zeros and the problem is not very small (less\n",
      "    than about 100 constraints or variables), consider setting ``True``\n",
      "    or providing ``A_eq`` and ``A_ub`` as sparse matrices.\n",
      "lstsq : bool (default = False)\n",
      "    Set to ``True`` if the problem is expected to be very poorly\n",
      "    conditioned. This should always be left ``False`` unless severe\n",
      "    numerical difficulties are encountered. Leave this at the default\n",
      "    unless you receive a warning message suggesting otherwise.\n",
      "sym_pos : bool (default = True)\n",
      "    Leave ``True`` if the problem is expected to yield a well conditioned\n",
      "    symmetric positive definite normal equation matrix\n",
      "    (almost always). Leave this at the default unless you receive\n",
      "    a warning message suggesting otherwise.\n",
      "cholesky : bool (default = True)\n",
      "    Set to ``True`` if the normal equations are to be solved by explicit\n",
      "    Cholesky decomposition followed by explicit forward/backward\n",
      "    substitution. This is typically faster for moderate, dense problems\n",
      "    that are numerically well-behaved.\n",
      "pc : bool (default = True)\n",
      "    Leave ``True`` if the predictor-corrector method of Mehrota is to be\n",
      "    used. This is almost always (if not always) beneficial.\n",
      "ip : bool (default = False)\n",
      "    Set to ``True`` if the improved initial point suggestion due to [1]_\n",
      "    Section 4.3 is desired. Whether this is beneficial or not\n",
      "    depends on the problem.\n",
      "presolve : bool (default = True)\n",
      "    Leave ``True`` if presolve routine should be run. The presolve routine\n",
      "    is almost always useful because it can detect trivial infeasibilities\n",
      "    and unboundedness, eliminate fixed variables, and remove redundancies.\n",
      "    One circumstance in which it might be turned off (set ``False``) is\n",
      "    when it detects that the problem is trivially unbounded; it is possible\n",
      "    that that the problem is truly infeasibile but this has not been\n",
      "    detected.\n",
      "rr : bool (default = True)\n",
      "    Default ``True`` attempts to eliminate any redundant rows in ``A_eq``.\n",
      "    Set ``False`` if ``A_eq`` is known to be of full row rank, or if you\n",
      "    are looking for a potential speedup (at the expense of reliability).\n",
      "permc_spec : str (default = 'MMD_AT_PLUS_A')\n",
      "    (Has effect only with ``sparse = True``, ``lstsq = False``, ``sym_pos =\n",
      "    True``.) A matrix is factorized in each iteration of the algorithm.\n",
      "    This option specifies how to permute the columns of the matrix for\n",
      "    sparsity preservation. Acceptable values are:\n",
      "\n",
      "    - ``NATURAL``: natural ordering.\n",
      "    - ``MMD_ATA``: minimum degree ordering on the structure of A^T A.\n",
      "    - ``MMD_AT_PLUS_A``: minimum degree ordering on the structure of A^T+A.\n",
      "    - ``COLAMD``: approximate minimum degree column ordering.\n",
      "\n",
      "    This option can impact the convergence of the\n",
      "    interior point algorithm; test different values to determine which\n",
      "    performs best for your problem. For more information, refer to\n",
      "    ``scipy.sparse.linalg.splu``.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "A ``scipy.optimize.OptimizeResult`` consisting of the following fields:\n",
      "\n",
      "    x : ndarray\n",
      "        The independent variable vector which optimizes the linear\n",
      "        programming problem.\n",
      "    fun : float\n",
      "        The optimal value of the objective function\n",
      "    con : float\n",
      "        The residuals of the equality constraints (nominally zero).\n",
      "    slack : ndarray\n",
      "        The values of the slack variables.  Each slack variable corresponds\n",
      "        to an inequality constraint.  If the slack is zero, then the\n",
      "        corresponding constraint is active.\n",
      "    success : bool\n",
      "        Returns True if the algorithm succeeded in finding an optimal\n",
      "        solution.\n",
      "    status : int\n",
      "        An integer representing the exit status of the optimization::\n",
      "\n",
      "             0 : Optimization terminated successfully\n",
      "             1 : Iteration limit reached\n",
      "             2 : Problem appears to be infeasible\n",
      "             3 : Problem appears to be unbounded\n",
      "             4 : Serious numerical difficulties encountered\n",
      "\n",
      "    nit : int\n",
      "        The number of iterations performed.\n",
      "    message : str\n",
      "        A string descriptor of the exit status of the optimization.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "\n",
      "This method implements the algorithm outlined in [1]_ with ideas from [5]_\n",
      "and a structure inspired by the simpler methods of [3]_ and [4]_.\n",
      "\n",
      "First, a presolve procedure based on [5]_ attempts to identify trivial\n",
      "infeasibilities, trivial unboundedness, and potential problem\n",
      "simplifications. Specifically, it checks for:\n",
      "\n",
      "- rows of zeros in ``A_eq`` or ``A_ub``, representing trivial constraints;\n",
      "- columns of zeros in ``A_eq`` `and` ``A_ub``, representing unconstrained\n",
      "  variables;\n",
      "- column singletons in ``A_eq``, representing fixed variables; and\n",
      "- column singletons in ``A_ub``, representing simple bounds.\n",
      "\n",
      "If presolve reveals that the problem is unbounded (e.g. an unconstrained\n",
      "and unbounded variable has negative cost) or infeasible (e.g. a row of\n",
      "zeros in ``A_eq`` corresponds with a nonzero in ``b_eq``), the solver\n",
      "terminates with the appropriate status code. Note that presolve terminates\n",
      "as soon as any sign of unboundedness is detected; consequently, a problem\n",
      "may be reported as unbounded when in reality the problem is infeasible\n",
      "(but infeasibility has not been detected yet). Therefore, if the output\n",
      "message states that unboundedness is detected in presolve and it is\n",
      "necessary to know whether the problem is actually infeasible, set option\n",
      "``presolve=False``.\n",
      "\n",
      "If neither infeasibility nor unboundedness are detected in a single pass\n",
      "of the presolve check, bounds are tightened where possible and fixed\n",
      "variables are removed from the problem. Then, linearly dependent rows\n",
      "of the ``A_eq`` matrix are removed, (unless they represent an\n",
      "infeasibility) to avoid numerical difficulties in the primary solve\n",
      "routine. Note that rows that are nearly linearly dependent (within a\n",
      "prescibed tolerance) may also be removed, which can change the optimal\n",
      "solution in rare cases. If this is a concern, eliminate redundancy from\n",
      "your problem formulation and run with option ``rr=False`` or\n",
      "``presolve=False``.\n",
      "\n",
      "Several potential improvements can be made here: additional presolve\n",
      "checks outlined in [5]_ should be implemented, the presolve routine should\n",
      "be run multiple times (until no further simplifications can be made), and\n",
      "more of the efficiency improvements from [2]_ should be implemented in the\n",
      "redundancy removal routines.\n",
      "\n",
      "After presolve, the problem is transformed to standard form by converting\n",
      "the (tightened) simple bounds to upper bound constraints, introducing\n",
      "non-negative slack variables for inequality constraints, and expressing\n",
      "unbounded variables as the difference between two non-negative variables.\n",
      "\n",
      "The primal-dual path following method begins with initial 'guesses' of\n",
      "the primal and dual variables of the standard form problem and iteratively\n",
      "attempts to solve the (nonlinear) Karush-Kuhn-Tucker conditions for the\n",
      "problem with a gradually reduced logarithmic barrier term added to the\n",
      "objective. This particular implementation uses a homogeneous self-dual\n",
      "formulation, which provides certificates of infeasibility or unboundedness\n",
      "where applicable.\n",
      "\n",
      "The default initial point for the primal and dual variables is that\n",
      "defined in [1]_ Section 4.4 Equation 8.22. Optionally (by setting initial\n",
      "point option ``ip=True``), an alternate (potentially improved) starting\n",
      "point can be calculated according to the additional recommendations of\n",
      "[1]_ Section 4.4.\n",
      "\n",
      "A search direction is calculated using the predictor-corrector method\n",
      "(single correction) proposed by Mehrota and detailed in [1]_ Section 4.1.\n",
      "(A potential improvement would be to implement the method of multiple\n",
      "corrections described in [1]_ Section 4.2.) In practice, this is\n",
      "accomplished by solving the normal equations, [1]_ Section 5.1 Equations\n",
      "8.31 and 8.32, derived from the Newton equations [1]_ Section 5 Equations\n",
      "8.25 (compare to [1]_ Section 4 Equations 8.6-8.8). The advantage of\n",
      "solving the normal equations rather than 8.25 directly is that the\n",
      "matrices involved are symmetric positive definite, so Cholesky\n",
      "decomposition can be used rather than the more expensive LU factorization.\n",
      "\n",
      "With the default ``cholesky=True``, this is accomplished using\n",
      "``scipy.linalg.cho_factor`` followed by forward/backward substitutions\n",
      "via ``scipy.linalg.cho_solve``. With ``cholesky=False`` and\n",
      "``sym_pos=True``, Cholesky decomposition is performed instead by\n",
      "``scipy.linalg.solve``. Based on speed tests, this also appears to retain\n",
      "the Cholesky decomposition of the matrix for later use, which is beneficial\n",
      "as the same system is solved four times with different right hand sides\n",
      "in each iteration of the algorithm.\n",
      "\n",
      "In problems with redundancy (e.g. if presolve is turned off with option\n",
      "``presolve=False``) or if the matrices become ill-conditioned (e.g. as the\n",
      "solution is approached and some decision variables approach zero),\n",
      "Cholesky decomposition can fail. Should this occur, successively more\n",
      "robust solvers (``scipy.linalg.solve`` with ``sym_pos=False`` then\n",
      "``scipy.linalg.lstsq``) are tried, at the cost of computational efficiency.\n",
      "These solvers can be used from the outset by setting the options\n",
      "``sym_pos=False`` and ``lstsq=True``, respectively.\n",
      "\n",
      "Note that with the option ``sparse=True``, the normal equations are solved\n",
      "using ``scipy.sparse.linalg.spsolve``. Unfortunately, this uses the more\n",
      "expensive LU decomposition from the outset, but for large, sparse problems,\n",
      "the use of sparse linear algebra techniques improves the solve speed\n",
      "despite the use of LU rather than Cholesky decomposition. A simple\n",
      "improvement would be to use the sparse Cholesky decomposition of\n",
      "``CHOLMOD`` via ``scikit-sparse`` when available.\n",
      "\n",
      "Other potential improvements for combatting issues associated with dense\n",
      "columns in otherwise sparse problems are outlined in [1]_ Section 5.3 and\n",
      "[7]_ Section 4.1-4.2; the latter also discusses the alleviation of\n",
      "accuracy issues associated with the substitution approach to free\n",
      "variables.\n",
      "\n",
      "After calculating the search direction, the maximum possible step size\n",
      "that does not activate the non-negativity constraints is calculated, and\n",
      "the smaller of this step size and unity is applied (as in [1]_ Section\n",
      "4.1.) [1]_ Section 4.3 suggests improvements for choosing the step size.\n",
      "\n",
      "The new point is tested according to the termination conditions of [1]_\n",
      "Section 4.5. The same tolerance, which can be set using the ``tol`` option,\n",
      "is used for all checks. (A potential improvement would be to expose\n",
      "the different tolerances to be set independently.) If optimality,\n",
      "unboundedness, or infeasibility is detected, the solve procedure\n",
      "terminates; otherwise it repeats.\n",
      "\n",
      "If optimality is achieved, a postsolve procedure undoes transformations\n",
      "associated with presolve and converting to standard form. It then\n",
      "calculates the residuals (equality constraint violations, which should\n",
      "be very small) and slacks (difference between the left and right hand\n",
      "sides of the upper bound constraints) of the original problem, which are\n",
      "returned with the solution in an ``OptimizeResult`` object.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] Andersen, Erling D., and Knud D. Andersen. \"The MOSEK interior point\n",
      "       optimizer for linear programming: an implementation of the\n",
      "       homogeneous algorithm.\" High performance optimization. Springer US,\n",
      "       2000. 197-232.\n",
      ".. [2] Andersen, Erling D. \"Finding all linearly dependent rows in\n",
      "       large-scale linear programming.\" Optimization Methods and Software\n",
      "       6.3 (1995): 219-227.\n",
      ".. [3] Freund, Robert M. \"Primal-Dual Interior-Point Methods for Linear\n",
      "       Programming based on Newton's Method.\" Unpublished Course Notes,\n",
      "       March 2004. Available 2/25/2017 at\n",
      "       https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec14_int_pt_mthd.pdf\n",
      ".. [4] Fourer, Robert. \"Solving Linear Programs by Interior-Point Methods.\"\n",
      "       Unpublished Course Notes, August 26, 2005. Available 2/25/2017 at\n",
      "       http://www.4er.org/CourseNotes/Book%20B/B-III.pdf\n",
      ".. [5] Andersen, Erling D., and Knud D. Andersen. \"Presolving in linear\n",
      "       programming.\" Mathematical Programming 71.2 (1995): 221-245.\n",
      ".. [6] Bertsimas, Dimitris, and J. Tsitsiklis. \"Introduction to linear\n",
      "       programming.\" Athena Scientific 1 (1997): 997.\n",
      ".. [7] Andersen, Erling D., et al. Implementation of interior point methods\n",
      "       for large scale linear programming. HEC/Universite de Geneve, 1996.\n"
     ]
    }
   ],
   "source": [
    "optimize.show_options(method='BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tbj]",
   "language": "python",
   "name": "conda-env-tbj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
